# 线性回归与神经网络

很多人以为“神经网络”很复杂，其实它最早是从“线性回归”发展而来的。

## 1. 线性回归

$$
y = wx + b
$$

我们通过最小化损失函数来训练模型：

$$
L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

## 2. 激活函数

引入非线性：

- ReLU: $ f(x) = \max(0, x) $
- Sigmoid: $ f(x) = \frac{1}{1 + e^{-x}} $

[← 返回首页](README.md)